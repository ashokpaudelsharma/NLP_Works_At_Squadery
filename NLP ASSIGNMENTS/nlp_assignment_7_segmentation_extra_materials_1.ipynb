{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd1c53bd",
   "metadata": {},
   "source": [
    "# Sentence segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd413aae",
   "metadata": {},
   "source": [
    "Sentence segmentation or sentence boundary disambiguation is one of the most crucial part in NLP, where a corpus of text is separated based on the sentences. Other languages such as English, Spanish, etc. use a reasonable approximation of delimiter (full-stop/comma) or case discrimination helps in detecting a sentence boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5f9544",
   "metadata": {},
   "source": [
    "Tokenization and sentence segmentation in Stanza are jointly performed by the TokenizeProcessor. This processor splits the raw input text into tokens and sentences, so that downstream annotation can happen at the sentence level. This processor can be invoked by the name tokenize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c498f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting stanza\n",
      "  Downloading stanza-1.4.0-py3-none-any.whl (574 kB)\n",
      "Requirement already satisfied: torch>=1.3.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from stanza) (1.8.2+cpu)\n",
      "Requirement already satisfied: six in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from stanza) (1.16.0)\n",
      "Collecting emoji\n",
      "  Downloading emoji-1.7.0.tar.gz (175 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from stanza) (1.21.5)\n",
      "Requirement already satisfied: protobuf in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from stanza) (3.19.1)\n",
      "Requirement already satisfied: transformers in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from stanza) (4.19.4)\n",
      "Requirement already satisfied: requests in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from stanza) (2.27.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from stanza) (4.64.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from torch>=1.3.0->stanza) (4.1.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests->stanza) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests->stanza) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests->stanza) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests->stanza) (1.26.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from tqdm->stanza) (0.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from transformers->stanza) (21.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from transformers->stanza) (0.7.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from transformers->stanza) (2022.3.15)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from transformers->stanza) (0.12.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from transformers->stanza) (3.6.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from transformers->stanza) (6.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers->stanza) (3.0.4)\n",
      "Building wheels for collected packages: emoji\n",
      "  Building wheel for emoji (setup.py): started\n",
      "  Building wheel for emoji (setup.py): finished with status 'done'\n",
      "  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171046 sha256=777c7e1efe7cfaf06545cdb314cc3227df89271cc9d15d8631ce488611c426aa\n",
      "  Stored in directory: c:\\users\\lenovo\\appdata\\local\\pip\\cache\\wheels\\fa\\7a\\e9\\22dd0515e1bad255e51663ee513a2fa839c95934c5fc301090\n",
      "Successfully built emoji\n",
      "Installing collected packages: emoji, stanza\n",
      "Successfully installed emoji-1.7.0 stanza-1.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "597a56ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2151c6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7e67d3f3e87405eb23e86f9fc66e8e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9782d657cbb41b393e0a5c21a61b34d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.4.0/models/tokenize/combined.pt:   0%|    …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-21 16:15:30 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "========================\n",
      "\n",
      "2022-06-21 16:15:30 INFO: Use device: cpu\n",
      "2022-06-21 16:15:30 INFO: Loading: tokenize\n",
      "2022-06-21 16:15:31 INFO: Done loading processors!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\n",
       "  [\n",
       "    {\n",
       "      \"id\": 1,\n",
       "      \"text\": \"This\",\n",
       "      \"start_char\": 0,\n",
       "      \"end_char\": 4\n",
       "    },\n",
       "    {\n",
       "      \"id\": 2,\n",
       "      \"text\": \"is\",\n",
       "      \"start_char\": 5,\n",
       "      \"end_char\": 7\n",
       "    },\n",
       "    {\n",
       "      \"id\": 3,\n",
       "      \"text\": \"a\",\n",
       "      \"start_char\": 8,\n",
       "      \"end_char\": 9\n",
       "    },\n",
       "    {\n",
       "      \"id\": 4,\n",
       "      \"text\": \"test\",\n",
       "      \"start_char\": 10,\n",
       "      \"end_char\": 14\n",
       "    },\n",
       "    {\n",
       "      \"id\": 5,\n",
       "      \"text\": \"sentence\",\n",
       "      \"start_char\": 15,\n",
       "      \"end_char\": 23\n",
       "    },\n",
       "    {\n",
       "      \"id\": 6,\n",
       "      \"text\": \"for\",\n",
       "      \"start_char\": 24,\n",
       "      \"end_char\": 27\n",
       "    },\n",
       "    {\n",
       "      \"id\": 7,\n",
       "      \"text\": \"stanza\",\n",
       "      \"start_char\": 28,\n",
       "      \"end_char\": 34\n",
       "    },\n",
       "    {\n",
       "      \"id\": 8,\n",
       "      \"text\": \".\",\n",
       "      \"start_char\": 34,\n",
       "      \"end_char\": 35\n",
       "    }\n",
       "  ],\n",
       "  [\n",
       "    {\n",
       "      \"id\": 1,\n",
       "      \"text\": \"This\",\n",
       "      \"start_char\": 36,\n",
       "      \"end_char\": 40\n",
       "    },\n",
       "    {\n",
       "      \"id\": 2,\n",
       "      \"text\": \"is\",\n",
       "      \"start_char\": 41,\n",
       "      \"end_char\": 43\n",
       "    },\n",
       "    {\n",
       "      \"id\": 3,\n",
       "      \"text\": \"another\",\n",
       "      \"start_char\": 44,\n",
       "      \"end_char\": 51\n",
       "    },\n",
       "    {\n",
       "      \"id\": 4,\n",
       "      \"text\": \"sentence\",\n",
       "      \"start_char\": 52,\n",
       "      \"end_char\": 60\n",
       "    },\n",
       "    {\n",
       "      \"id\": 5,\n",
       "      \"text\": \".\",\n",
       "      \"start_char\": 60,\n",
       "      \"end_char\": 61\n",
       "    }\n",
       "  ]\n",
       "]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(lang='en', processors='tokenize')\n",
    "doc = nlp('This is a test sentence for stanza. This is another sentence.')\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8df9ec97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Sentence 1 tokens =======\n",
      "id: (1,)\ttext: This\n",
      "id: (2,)\ttext: is\n",
      "id: (3,)\ttext: a\n",
      "id: (4,)\ttext: test\n",
      "id: (5,)\ttext: sentence\n",
      "id: (6,)\ttext: for\n",
      "id: (7,)\ttext: stanza\n",
      "id: (8,)\ttext: .\n",
      "====== Sentence 2 tokens =======\n",
      "id: (1,)\ttext: This\n",
      "id: (2,)\ttext: is\n",
      "id: (3,)\ttext: another\n",
      "id: (4,)\ttext: sentence\n",
      "id: (5,)\ttext: .\n"
     ]
    }
   ],
   "source": [
    "for i, sentence in enumerate(doc.sentences):\n",
    "    print(f'====== Sentence {i+1} tokens =======')\n",
    "    print(*[f'id: {token.id}\\ttext: {token.text}' for token in sentence.tokens], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "357c5c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is a test sentence for stanza.', 'This is another sentence.']\n"
     ]
    }
   ],
   "source": [
    "#Get individual sentences from the doc\n",
    "print([sentence.text for sentence in doc.sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d871bc60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ea477921ca345d89846dc797560229e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-21 16:28:10 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "========================\n",
      "\n",
      "2022-06-21 16:28:10 INFO: Use device: cpu\n",
      "2022-06-21 16:28:10 INFO: Loading: tokenize\n",
      "2022-06-21 16:28:11 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Sentence 1 tokens =======\n",
      "id: (1,)\ttext: This\n",
      "id: (2,)\ttext: is\n",
      "id: (3,)\ttext: a\n",
      "id: (4,)\ttext: sentence\n",
      "id: (5,)\ttext: .\n",
      "id: (6,)\ttext: This\n",
      "id: (7,)\ttext: is\n",
      "id: (8,)\ttext: a\n",
      "id: (9,)\ttext: second\n",
      "id: (10,)\ttext: .\n",
      "====== Sentence 2 tokens =======\n",
      "id: (1,)\ttext: This\n",
      "id: (2,)\ttext: is\n",
      "id: (3,)\ttext: a\n",
      "id: (4,)\ttext: third\n",
      "id: (5,)\ttext: .\n"
     ]
    }
   ],
   "source": [
    "#Tokenization without sentence segmentation\n",
    "\n",
    "nlp = stanza.Pipeline(lang='en', processors='tokenize', tokenize_no_ssplit=True)\n",
    "doc = nlp('This is a sentence.This is a second.\\n\\nThis is a third.')\n",
    "for i, sentence in enumerate(doc.sentences):\n",
    "    print(f'====== Sentence {i+1} tokens =======')\n",
    "    print(*[f'id: {token.id}\\ttext: {token.text}' for token in sentence.tokens], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528ba1c1",
   "metadata": {},
   "source": [
    "Note: Here, \"This is a second. This is a third.\" were supposed to be 2 different sentences, but, they were not segmented because of the condition, \"tokenize_no_ssplit=True\". But, whenever \"\\n\\n\" are mentioned explicitly (single '\\n' doesn't work, two \\n's are necessary), the segmentation still happens though.\n",
    "Following is what happens if \"tokenize_no_ssplit=False\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "081f26e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fafa247843ce45d9b797ba09e817e5b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-21 16:30:49 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "========================\n",
      "\n",
      "2022-06-21 16:30:49 INFO: Use device: cpu\n",
      "2022-06-21 16:30:49 INFO: Loading: tokenize\n",
      "2022-06-21 16:30:49 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Sentence 1 tokens =======\n",
      "id: (1,)\ttext: This\n",
      "id: (2,)\ttext: is\n",
      "id: (3,)\ttext: a\n",
      "id: (4,)\ttext: sentence\n",
      "id: (5,)\ttext: .\n",
      "====== Sentence 2 tokens =======\n",
      "id: (1,)\ttext: This\n",
      "id: (2,)\ttext: is\n",
      "id: (3,)\ttext: a\n",
      "id: (4,)\ttext: second\n",
      "id: (5,)\ttext: .\n",
      "====== Sentence 3 tokens =======\n",
      "id: (1,)\ttext: This\n",
      "id: (2,)\ttext: is\n",
      "id: (3,)\ttext: a\n",
      "id: (4,)\ttext: third\n",
      "id: (5,)\ttext: .\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(lang='en', processors='tokenize', tokenize_no_ssplit=False)\n",
    "doc = nlp('This is a sentence.This is a second.\\n\\nThis is a third.')\n",
    "for i, sentence in enumerate(doc.sentences):\n",
    "    print(f'====== Sentence {i+1} tokens =======')\n",
    "    print(*[f'id: {token.id}\\ttext: {token.text}' for token in sentence.tokens], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68acee94",
   "metadata": {},
   "source": [
    "Note: In this case, both sentence segmentation and tokenization occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "46217518",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "000832f7c75649039020ef354d240155",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-21 16:43:15 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "========================\n",
      "\n",
      "2022-06-21 16:43:15 INFO: Use device: cpu\n",
      "2022-06-21 16:43:15 INFO: Loading: tokenize\n",
      "2022-06-21 16:43:15 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Sentence 1 tokens =======\n",
      "id: (1,)\ttext: This\n",
      "id: (2,)\ttext: is\n",
      "id: (3,)\ttext: token.ization\n",
      "id: (4,)\ttext: done\n",
      "id: (5,)\ttext: my\n",
      "id: (6,)\ttext: way!\n",
      "id: (7,)\ttext: Sentence\n",
      "id: (8,)\ttext: are\n",
      "id: (9,)\ttext: not\n",
      "id: (10,)\ttext: split\n",
      "id: (11,)\ttext: if\n",
      "id: (12,)\ttext: no\n",
      "id: (13,)\ttext: new\n",
      "id: (14,)\ttext: line!\n",
      "====== Sentence 2 tokens =======\n",
      "id: (1,)\ttext: New\n",
      "id: (2,)\ttext: line\n",
      "id: (3,)\ttext: splits\n",
      "id: (4,)\ttext: sentence.\n"
     ]
    }
   ],
   "source": [
    "#Case of Pre-Tokenized Text\n",
    "\n",
    "nlp = stanza.Pipeline(lang='en', processors='tokenize', tokenize_pretokenized=True)\n",
    "doc = nlp('This is token.ization done my way! Sentence are not split if no new line! \\n New line splits sentence.')\n",
    "for i, sentence in enumerate(doc.sentences):\n",
    "    print(f'====== Sentence {i+1} tokens =======')\n",
    "    print(*[f'id: {token.id}\\ttext: {token.text}' for token in sentence.tokens], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1e2ae0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "55d42369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5a77a805fd147b8aec9cc34e1d427c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-21 16:42:19 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "========================\n",
      "\n",
      "2022-06-21 16:42:19 INFO: Use device: cpu\n",
      "2022-06-21 16:42:19 INFO: Loading: tokenize\n",
      "2022-06-21 16:42:19 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Sentence 1 tokens =======\n",
      "id: (1,)\ttext: This\n",
      "id: (2,)\ttext: is\n",
      "id: (3,)\ttext: token.ization\n",
      "id: (4,)\ttext: done\n",
      "id: (5,)\ttext: my\n",
      "id: (6,)\ttext: way\n",
      "id: (7,)\ttext: !\n",
      "====== Sentence 2 tokens =======\n",
      "id: (1,)\ttext: Sentence\n",
      "id: (2,)\ttext: split\n",
      "id: (3,)\ttext: ,\n",
      "id: (4,)\ttext: too\n",
      "id: (5,)\ttext: !\n"
     ]
    }
   ],
   "source": [
    "#Case of Not Pre-Tokenized Text\n",
    "\n",
    "nlp = stanza.Pipeline(lang='en', processors='tokenize', tokenize_pretokenized=False)\n",
    "doc = nlp('This is token.ization done my way! \\n Sentence split, too!')\n",
    "for i, sentence in enumerate(doc.sentences):\n",
    "    print(f'====== Sentence {i+1} tokens =======')\n",
    "    print(*[f'id: {token.id}\\ttext: {token.text}' for token in sentence.tokens], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65292d7",
   "metadata": {},
   "source": [
    "Note: Here , ',' and '!' are separate tokens in contrast to previous case where ,',' and '!' are attached to words, i.e, 'split,', 'way!' and 'too!'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f1ab6602",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73b0989971e74b70a8b05819fb3a5299",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-21 16:40:10 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "========================\n",
      "\n",
      "2022-06-21 16:40:10 INFO: Use device: cpu\n",
      "2022-06-21 16:40:10 INFO: Loading: tokenize\n",
      "2022-06-21 16:40:10 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Sentence 1 tokens =======\n",
      "id: (1,)\ttext: This\n",
      "id: (2,)\ttext: is\n",
      "id: (3,)\ttext: token.ization\n",
      "id: (4,)\ttext: done\n",
      "id: (5,)\ttext: my\n",
      "id: (6,)\ttext: way!\n",
      "====== Sentence 2 tokens =======\n",
      "id: (1,)\ttext: Sentence\n",
      "id: (2,)\ttext: split,\n",
      "id: (3,)\ttext: too!\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(lang='en', processors='tokenize', tokenize_pretokenized=True)\n",
    "doc = nlp([['This', 'is', 'token.ization', 'done', 'my', 'way!'], ['Sentence', 'split,', 'too!']])\n",
    "for i, sentence in enumerate(doc.sentences):\n",
    "    print(f'====== Sentence {i+1} tokens =======')\n",
    "    print(*[f'id: {token.id}\\ttext: {token.text}' for token in sentence.tokens], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2850534",
   "metadata": {},
   "source": [
    "# Use spaCy for Fast Tokenization and Sentence Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aa3dc937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.3.1-cp39-cp39-win_amd64.whl (11.7 MB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.7-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from spacy) (61.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from spacy) (21.3)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "Collecting pathy>=0.3.5\n",
      "  Downloading pathy-0.6.1-py3-none-any.whl (42 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from spacy) (2.11.3)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.7-cp39-cp39-win_amd64.whl (18 kB)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
      "  Downloading pydantic-1.8.2-cp39-cp39-win_amd64.whl (1.9 MB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Downloading spacy_loggers-1.0.2-py3-none-any.whl (7.2 kB)\n",
      "Collecting typer<0.5.0,>=0.3.0\n",
      "  Downloading typer-0.4.1-py3-none-any.whl (27 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.9\n",
      "  Downloading spacy_legacy-3.0.9-py2.py3-none-any.whl (20 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.6-cp39-cp39-win_amd64.whl (112 kB)\n",
      "Collecting blis<0.8.0,>=0.4.0\n",
      "  Downloading blis-0.7.7-cp39-cp39-win_amd64.whl (6.6 MB)\n",
      "Collecting srsly<3.0.0,>=2.4.3\n",
      "  Downloading srsly-2.4.3-cp39-cp39-win_amd64.whl (448 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from spacy) (4.64.0)\n",
      "Collecting thinc<8.1.0,>=8.0.14\n",
      "  Downloading thinc-8.0.17-cp39-cp39-win_amd64.whl (1.0 MB)\n",
      "Collecting wasabi<1.1.0,>=0.9.1\n",
      "  Downloading wasabi-0.9.1-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from spacy) (1.21.5)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.6-cp39-cp39-win_amd64.whl (36 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from spacy) (2.27.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy) (5.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy) (4.1.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
      "Requirement already satisfied: colorama in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.0.1)\n",
      "Installing collected packages: murmurhash, cymem, catalogue, wasabi, typer, srsly, pydantic, preshed, blis, thinc, spacy-loggers, spacy-legacy, pathy, langcodes, spacy\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 1.9.1\n",
      "    Uninstalling pydantic-1.9.1:\n",
      "      Successfully uninstalled pydantic-1.9.1\n",
      "Successfully installed blis-0.7.7 catalogue-2.0.7 cymem-2.0.6 langcodes-3.3.0 murmurhash-1.0.7 pathy-0.6.1 preshed-3.0.6 pydantic-1.8.2 spacy-3.3.1 spacy-legacy-3.0.9 spacy-loggers-1.0.2 srsly-2.4.3 thinc-8.0.17 typer-0.4.1 wasabi-0.9.1\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f71b8db6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6d020b858534bc0b0eb5f366029b0bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-21 16:52:28 INFO: Loading these models for language: en (English):\n",
      "============================\n",
      "| Processor    | Package   |\n",
      "----------------------------\n",
      "| tokenize     | spacy     |\n",
      "| pos          | combined  |\n",
      "| lemma        | combined  |\n",
      "| depparse     | combined  |\n",
      "| sentiment    | sstplus   |\n",
      "| constituency | wsj       |\n",
      "| ner          | ontonotes |\n",
      "============================\n",
      "\n",
      "2022-06-21 16:52:28 INFO: Use device: cpu\n",
      "2022-06-21 16:52:28 INFO: Loading: tokenize\n",
      "2022-06-21 16:52:32 INFO: Loading: pos\n",
      "2022-06-21 16:52:33 INFO: Loading: lemma\n",
      "2022-06-21 16:52:33 INFO: Loading: depparse\n",
      "2022-06-21 16:52:33 INFO: Loading: sentiment\n",
      "2022-06-21 16:52:33 INFO: Loading: constituency\n",
      "2022-06-21 16:52:34 INFO: Loading: ner\n",
      "2022-06-21 16:52:35 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Sentence 1 tokens =======\n",
      "id: (1,)\ttext: This\n",
      "id: (2,)\ttext: is\n",
      "id: (3,)\ttext: a\n",
      "id: (4,)\ttext: test\n",
      "id: (5,)\ttext: sentence\n",
      "id: (6,)\ttext: for\n",
      "id: (7,)\ttext: stanza\n",
      "id: (8,)\ttext: .\n",
      "====== Sentence 2 tokens =======\n",
      "id: (1,)\ttext: This\n",
      "id: (2,)\ttext: is\n",
      "id: (3,)\ttext: another\n",
      "id: (4,)\ttext: sentence\n",
      "id: (5,)\ttext: .\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(lang='en', processors={'tokenize': 'spacy'}) # spaCy tokenizer is currently only allowed in English pipeline.\n",
    "doc = nlp('This is a test sentence for stanza. This is another sentence.')\n",
    "for i, sentence in enumerate(doc.sentences):\n",
    "    print(f'====== Sentence {i+1} tokens =======')\n",
    "    print(*[f'id: {token.id}\\ttext: {token.text}' for token in sentence.tokens], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a1233e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
